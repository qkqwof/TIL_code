{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train.ipynb","private_outputs":true,"provenance":[{"file_id":"1RS4fN9fkHZWYIWZtKfJmT51okNKaYUi_","timestamp":1600920618999}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"xvpWJPAFTV8B"},"source":["# Mount Google Drive Folder (/content/drive/My Drive/Colab)\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# import user defined files \n","import sys\n","import_dir =\"/content/drive/My Drive/Colab Notebooks/3. Semantic Segmentation\"\n","sys.path.insert(0, import_dir)\n","\n","# import ipynb files\n","!pip install import_ipynb \n","import import_ipynb\n","\n","# Import ipynb in different directory\n","#%run '/content/drive/My Drive/Colab Notebooks/3. Semantic Segmentation/dataset.ipynb'\n","\n","# import easydict instead of using argparse (argparse is not usuable for ipynb)\n","import easydict \n","global _args\n","_args = easydict.EasyDict({\"config\": '/content/drive/My Drive/Colab Notebooks/3. Semantic Segmentation/config/isic.yaml'})\n","\n","# Debugging Tool\n","import pdb"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7nj3qeh5yJiz"},"source":["# Download & Unzip Dataset (Move from google drive to local)\n","%cp -r '/content/drive/My Drive/Colab Notebooks/Dataset/ISIC/ISIC.zip' '/content/sample_data'\n","!unzip '/content/sample_data/ISIC.zip' \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"udcJ52n4qj_D"},"source":["# Initilize Additional parameters\n","train_root = \"/content/ISIC_Trainset\"\n","test_root = \"/content/ISIC_Testset\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SzNTaX6rqnRo"},"source":["import os\n","import pdb\n","import argparse\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","\n","import models\n","from utils import dataset, config, utils, dataset_utils"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rhRlw7qTyJlE"},"source":["def get_parser(args): # args는 config파일 위치\n","    '''\n","    # Removed Code (Because ipynb file does not support argparse)\n","    # Even removed, these lines are substitued by easydict code\n","\n","    parser = argparse.ArgumentParser(description='PyTorch Semantic Segmentation')\n","    parser.add_argument('--config', type=str, default='config/cityscapes.yaml')\n","    args = parser.parse_args()\n","    '''\n","    \n","    assert args.config is not None # 예외처리 얘가 False가 되면 에러난다.\n","\n","    '''\n","    yaml파일 읽어서 key, value로 읽어들인 값을 {}에 집어넣고 그대로 딕셔너리 리턴하지 않고 \n","    하나를 더해준다.\n","    ConfigNode 클래스를 하나 더 만들고 딕셔너리 형태를 다시 argparse형태로 바꿔주는 부분\n","    '''\n","    cfg = config.load_cfg_from_cfg_file(args.config) # utils<config.py<load_cfg_from_cfg_file\n","    return cfg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9joYUTDTyJtf"},"source":["def check(args): #리스트 안에 있으면 True 없으면 False가 나고 에러로 떨어진다.\n","    assert args.data_name in ['crack', 'cityscapes', 'isic']\n","    assert args.classes >= 1\n","    assert args.model in ['unet32', 'unet64', 'unet128',\n","                          'deeplab34','deeplab50','deeplab101',\n","                          'pspnet18', 'pspnet34', 'pspnet50']\n","    assert args.loss in ['bce','ce','dice']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5LTc0Cu9yJro"},"source":["def main():\n","    args = get_parser(_args)    \n","    check(args) \n","    os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(str(x) for x in args.gpu) # gpu 세팅방법.\n","    main_worker(args)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7hGeTQDwyJnl"},"source":["def main_worker(argss):\n","    global args\n","    args = argss\n","\n","    ##### Model #####\n","\n","    model = models.load_model(args) \n","    pdb.set_trace()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=args.base_lr)\n","    \n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n","    best_loss = 999.0\n","\n","    if len(args.gpu) > 1:\n","        model = torch.nn.DataParallel(model.cuda())\n","    else:\n","        model = model.cuda()\n","    print(f\"=> creating {args.model} (classes:{args.classes})\")\n","\n","    os.makedirs(args.ckpt_root, exist_ok=True)\n","\n","    \n","    if args.weight: \n","        if os.path.isfile(args.weight):\n","            ckpt = torch.load(args.weight, map_location=lambda storage, loc: storage.cuda())\n","            args.start_epoch = ckpt['epoch']\n","            model.load_state_dict(ckpt['state_dict'], strict=False)\n","            optimizer.load_state_dict(ckpt['optimizer'])\n","            print(f\"=> loaded checkpoint '{args.weight}' (epoch {ckpt['epoch']})\") \n","        else:\n","            print(f\"=> no weight found at '{args.weight}'\")\n","\n","\n","    ##### Data #####\n","    criterion, transforms, _, _, _ = dataset_utils.dataset_utils(args)\n","\n","    train_loader = torch.utils.data.DataLoader(\n","                    dataset.Dataset(mode='train', data_root=args.train_root, data_list=args.train_list,\n","                                    transform=transforms[0], label_transform=transforms[3]),\n","                    batch_size=args.batch_size, shuffle=True, num_workers=args.workers,\n","                    pin_memory=True, drop_last=True)\n","\n","    val_loader = torch.utils.data.DataLoader(\n","                    dataset.Dataset(mode='val', data_root=args.val_root, data_list=args.val_list,\n","                                    transform=transforms[1], label_transform=transforms[3]),\n","                    batch_size=args.batch_size, shuffle=False, num_workers=args.workers,\n","                    pin_memory=True)\n","\n","\n","    ##### Train & Validation #####\n","    del_ckpt = None\n","    for epoch in range(args.start_epoch, args.epochs):\n","        loss_train = train(train_loader, model, criterion,\n","                                        optimizer, scheduler, epoch)\n","        loss_val = validate(val_loader, model, criterion, epoch)\n","\n","        if best_loss > loss_val:\n","            best_loss = loss_val\n","            if del_ckpt:\n","                os.remove(del_ckpt)\n","            filename = f'{args.ckpt_root}/{args.data_name}_{args.model}_{epoch+1}.pth'\n","            torch.save({'epoch': epoch+1,\n","                        'state_dict': model.state_dict(),\n","                        'optimizer': optimizer.state_dict()}, filename)\n","            del_ckpt = filename\n","            print(f'Saving checkpoint to: {filename}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fV1y3cMHrrbs"},"source":["def train(train_loader, model, criterion, optimizer, scheduler, epoch): # 이부분은 이미 알고 있는 코드이다.\n","    print(f'\\nMode: Train | Epoch: {epoch+1}/{args.epochs} | Model: {args.model} | Data: {args.data_name}')\n","    loss_meter = utils.AverageMeter()\n","\n","    torch.set_grad_enabled(True)\n","    model.train()\n","    for i, data in enumerate(train_loader):\n","        img, label = data[0].cuda(), data[1].cuda()\n","        output = model(img)\n","        loss = criterion(output, label.long())\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        loss_meter.update(loss.item(), img.size(0))\n","        # utils.progress_bar(i, len(train_loader), f'Loss:{loss_meter.avg:.4f}')\n","    print(f'Loss:{loss_meter.avg:.4f}')\n","    scheduler.step()\n","    torch.cuda.empty_cache()\n","    return loss_meter.avg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u123xSj9t-KG"},"source":["def validate(val_loader, model, criterion, epoch):\n","    print(f'\\nMode: Validation  |  Epoch: {epoch+1}/{args.epochs}  |  Model: {args.model}')\n","    loss_meter = utils.AverageMeter()\n","\n","    torch.set_grad_enabled(False)\n","    model.eval()\n","    for i, data in enumerate(val_loader):\n","        img, label = data[0].cuda(), data[1].cuda()\n","        output = model(img)\n","        loss = criterion(output, label.long())\n","        loss_meter.update(loss.item(), img.size(0))\n","        # utils.progress_bar(i, len(val_loader), f'Loss:{loss_meter.avg:.4f}')\n","    print(f'Loss:{loss_meter.avg:.4f}')\n","    torch.cuda.empty_cache()\n","    return loss_meter.avg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wSSNFJjXrqFo"},"source":["if __name__ == '__main__':\n","    main()"],"execution_count":null,"outputs":[]}]}