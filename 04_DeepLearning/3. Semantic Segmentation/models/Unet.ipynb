{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Unet.ipynb","private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNO5YZScN//53Fna+a9m22+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"sCN_iC0cHZmg"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import pdb\n","\n","class DoubleConv(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1):\n","        super().__init__()\n","        assert stride in [1, 2]\n","        self.double_conv = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True))\n","\n","    def forward(self, x):\n","        return self.double_conv(x)\n","\n","\n","class Down(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.conv = nn.Sequential(\n","            DoubleConv(in_channels, out_channels, stride=2))\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        return x\n","\n","\n","class Up(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.conv = DoubleConv(in_channels, out_channels)\n","\n","    def forward(self, x1, x2):\n","        h, w = x1.size()[2:]\n","        x1 = F.interpolate(x1, (h*2,w*2))\n","        x = torch.cat([x2, x1], dim=1)\n","        return self.conv(x)\n","\n","\n","class UNet(nn.Module):\n","    def __init__(self, classes, model):\n","        super(UNet, self).__init__()\n","        if model == 'unet32':\n","            base_channels=32\n","        elif model == 'unet64':\n","            base_channels=64\n","        elif model == 'unet128':\n","            base_channels=128\n","        else:\n","            raise ValueError(f'{model} is not supported model')\n","\n","        self.inc   = DoubleConv(3, base_channels) # Assume input has 3 channels\n","        self.down1 = Down(base_channels, base_channels*2)\n","        self.down2 = Down(base_channels*2, base_channels*4)\n","        self.down3 = Down(base_channels*4, base_channels*8)\n","        self.down4 = Down(base_channels*8, base_channels*8)\n","        self.up1   = Up(base_channels*16, base_channels*4)\n","        self.up2   = Up(base_channels*8, base_channels*2)\n","        self.up3   = Up(base_channels*4, base_channels)\n","        self.up4   = Up(base_channels*2, base_channels)\n","        self.outc  = nn.Conv2d(base_channels, classes, kernel_size=1)\n","\n","    def forward(self, x):\n","        x1 = self.inc(x)\n","        x2 = self.down1(x1)\n","        x3 = self.down2(x2)\n","        x4 = self.down3(x3)\n","        x5 = self.down4(x4)\n","        x = self.up1(x5, x4)\n","        x = self.up2(x, x3)\n","        x = self.up3(x, x2)\n","        x = self.up4(x, x1)\n","        x = self.outc(x)\n","        return x\n","\n","\n","if __name__ == \"__main__\":\n","    import pdb\n","\n","    model = UNet(2, 'unet128').cuda()\n","    x = torch.rand(4,3,256,256).cuda()\n","    y = model(x)\n","    print(y.size())"],"execution_count":null,"outputs":[]}]}