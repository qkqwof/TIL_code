{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"main.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"markdown","metadata":{"id":"R5plgQNfxxWY"},"source":["### 해당 코드는 모든 인공지능 코드의 뼈대이자 핵심이다.\n","###  이것만 잘 마스터 해두면 Segmentation 코드도 이해 가능하다."]},{"cell_type":"code","metadata":{"id":"-4fqi0NB_bsj"},"source":["# Mount Google Drive Folder (/content/drive/My Drive/Colab)\n","# colab에서 구글 드라이버를 사용하기 위해서 import 하는 부분\n","from google.colab import drive\n","\n","# 구글 드라이버의 데이터를 마운트 해오는 위치를 지정. content가 코랩의 로컬홈. 이 밑의 무조건 drive 하위로 잡아야 한다.\n","drive.mount('/content/drive')\n","\n","# import user defined files \n","import sys\n","import_dir =\"/content/drive/My Drive/Colab Notebooks/2. Classification/pytorch-cifar\"\n","sys.path.insert(0, import_dir)\n","\n","# import ipynb files...\n","!pip install import_ipynb \n","import import_ipynb\n","\n","%run '/content/drive/My Drive/Colab Notebooks/2. Classification/pytorch-cifar/dataset.ipynb'\n","\n","import easydict \n","# 파이썬에서 실행할 때 값입력 python main.py --lr 0.01 --resume False\n","# ipynb 라인기반으로 작동하기 떄문에 위처럼 값입력 안된다.\n","args = easydict.EasyDict({ \"lr\": 0.1, \"resume\": False})\n","\n","# Debugging Tool\n","import pdb\n","\n","# Download & Unzip Dataset (Move from google drive to local)\n","# 3. 구글 드라이브에 있는 cifar.tgz 압축파일을 카피해서 로컬(content/sample_data) 이 안으로 저장해오겠다.\n","%cp -r '/content/drive/My Drive/Colab Notebooks/Dataset/CIFAR10/cifar.tgz' '/content/sample_data'\n","\n","# 4. 코랩의 로컬 위치에서 (content/sample_data) 이 안에서 압축을 풀었다.\n","!tar -xvf  '/content/sample_data/cifar.tgz'\n","\n","# Initilize Additional parameters\n","train_root = \"/content/cifar/train\"\n","test_root = \"/content/cifar/test\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Q7zh5csGSS8"},"source":["'''Train CIFAR10 with PyTorch.'''\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.backends.cudnn as cudnn\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","import os\n","import argparse\n","\n","'''\n","from models import *\n","models 패키지에 있는 모든 걸 다 가져와라... *\n","models 안으로 들어가면 여러개의 파일이 있다.\n","1. __init__.py 이걸 먼저 찾게 되어 있다. 이게 제일 먼저 import 된다.\n","2. __init__.py 이 안으로 다시 들어가보자.\n","    우리가 선택해서 사용할 수 있는 여러 모델들이 다 나와있다.\n","    모델들이 import 되어진다.\n","\n","결론은 __init__이 먼저 import 되고 / __init__에 들어있는 것들이 나중에 import 되어지는 \n","이중적인 구조로 되어 있다.\n","''' \n","from models import *\n","from utils import progress_bar\n","\n","'''\n","# Removed Code (Because ipynb file does not support argparse)\n","# Even removed, these lines are substitued by easydict code\n","\n","parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n","parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n","parser.add_argument('--resume', '-r', action='store_true',\n","                    help='resume from checkpoint')\n","args = parser.parse_args()\n","'''\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","best_acc = 0  # best test accuracy\n","start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n","\n","# Data\n","print('==> Preparing data..')\n","# 데이터 변형(Data Augument)\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4), # 이미지 자르기\n","    transforms.RandomHorizontalFlip(), # 수평 뒤집기\n","    transforms.ToTensor(),\n","    #transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    #transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","# Modified for custom dataset\n","#trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n","trainset = Dataset(root=train_root, transforms=transform_train)\n","trainloader = torch.utils.data.DataLoader(\n","    trainset, batch_size=128, shuffle=True, num_workers=os.cpu_count(), drop_last=True) # num_workers : 데이터 로딩에 사용하는 subprocess개수\n","\n","# Modified for custom dataset\n","#testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n","testset = Dataset(root=test_root, transforms=transform_test)\n","testloader = torch.utils.data.DataLoader(\n","    testset, batch_size=100, shuffle=False, num_workers=os.cpu_count(), drop_last=True)\n","\n","classes = ('plane', 'car', 'bird', 'cat', 'deer',\n","           'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","print('==> Building model..')\n","# net = VGG('VGG19')\n","# net = ResNet18()\n","# net = PreActResNet18()\n","# net = GoogLeNet()\n","# net = DenseNet121()\n","# net = ResNeXt29_2x64d()\n","net = MobileNet() # 가장 가벼운 모델\n","# net = MobileNetV2()\n","# net = DPN92()\n","# net = ShuffleNetG2()\n","# net = SENet18()\n","# net = ShuffleNetV2(1)\n","# net = EfficientNetB0()\n","# net = RegNetX_200MF()\n","\n","\n","net = net.to(device)\n","\n","if device == 'cuda':\n","    net = torch.nn.DataParallel(net) # 모델을 병렬로 실행하여 다수의 GPU 에서 쉽게 작업을 실행\n","    cudnn.benchmark = True # 내장된 cudnn 자동 튜너를 활성화하여, 하드웨어에 맞게 사용할 최상의 알고리즘(텐서 크기나 conv 연산에 맞게?)을 찾는다.\n","\n","if args.resume:\n","    # Load checkpoint.\n","    print('==> Resuming from checkpoint..')   \n","    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!' # assert 조건, \"메시지\" -> 가정 설정문\n","    checkpoint = torch.load('./checkpoint/ckpt.pth')\n","    \n","    net.load_state_dict(checkpoint['net']) # 모델의 매개변수들을 불러옴/ state_dict:각 계층을 매개변수 텐서로 매핑되는 Python 사전(dict) 객체\n","    best_acc = checkpoint['acc']\n","    start_epoch = checkpoint['epoch']\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=args.lr,\n","                      momentum=0.9, weight_decay=5e-4)\n","\n","\n","# Training\n","def train(epoch):\n","    print('\\nEpoch: %d' % epoch)\n","    net.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0    \n","\n","    for batch_idx, (inputs, targets) in enumerate(trainloader): # \n","        inputs, targets = inputs.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item() \n","        _, predicted = outputs.max(1)        \n","        total += targets.size(0) \n","        correct += predicted.eq(targets).sum().item() # 객체 간의 비교 연산 eq() : '=='\n","\n","        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","        #print(batch_idx,'/', len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","        #             % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","\n","\n","def test(epoch):\n","    global best_acc\n","    net.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(testloader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = net(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            test_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","            #print(batch_idx,'/', len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","            #             % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","\n","    # Save checkpoint.   \n","    acc = 100.*correct/total\n","    if acc > best_acc:\n","        print('Saving..')\n","        state = {\n","            'net': net.state_dict(),\n","            'acc': acc,\n","            'epoch': epoch,\n","        }\n","        if not os.path.isdir('checkpoint'): # 경로가 존재하는지 확인하기\n","            os.mkdir('checkpoint') # 새로운 폴더 만들기\n","        torch.save(state, './checkpoint/ckpt.pth') \n","        best_acc = acc\n","\n","\n","\n","for epoch in range(start_epoch, start_epoch+20):\n","    train(epoch)\n","    test(epoch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T6zhoF8JVYSY"},"source":[""],"execution_count":null,"outputs":[]}]}