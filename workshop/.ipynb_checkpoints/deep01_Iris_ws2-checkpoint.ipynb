{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKLearn Iris Data Loader and DataFrame Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                  5.1               3.5                1.4               0.2\n",
       "1                  4.9               3.0                1.4               0.2\n",
       "2                  4.7               3.2                1.3               0.2\n",
       "3                  4.6               3.1                1.5               0.2\n",
       "4                  5.0               3.6                1.4               0.2\n",
       "..                 ...               ...                ...               ...\n",
       "145                6.7               3.0                5.2               2.3\n",
       "146                6.3               2.5                5.0               1.9\n",
       "147                6.5               3.0                5.2               2.0\n",
       "148                6.2               3.4                5.4               2.3\n",
       "149                5.9               3.0                5.1               1.8\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write Code !!\n",
    "iris = datasets.load_iris()\n",
    "iris\n",
    "\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X,y data Generator...Feature and Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "# 속성과 라벨을 X, y에 할당\n",
    "\n",
    "X, y = iris.data, iris.target\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Test 데이타를 8:2로 비율로 섞고, random_state=42로 지정\n",
    "    X_train, X_test, y_train, y_test 로 각각 할당된 값들을 torch 타입으로 변환 \n",
    "    torch.FloatTensor(), torch.LongTensor 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, \n",
    "                                                    iris.target, \n",
    "                                                    random_state=42,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=iris.target)\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 하이퍼파라미터 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4\n",
    "hidden_size = 56\n",
    "num_classes = 3\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NeuralNetwork  Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module): \n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    # 모델의 Forward Path를 정의\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NeuralNetwork  Model Excution , loss, optimizer, backward ..\n",
    "    Forward Propagation and Baward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [1/100], Loss : 2.2843\n",
      "Epoch : [2/100], Loss : 1.5035\n",
      "Epoch : [3/100], Loss : 1.0692\n",
      "Epoch : [4/100], Loss : 1.0203\n",
      "Epoch : [5/100], Loss : 1.0954\n",
      "Epoch : [6/100], Loss : 1.1309\n",
      "Epoch : [7/100], Loss : 1.1108\n",
      "Epoch : [8/100], Loss : 1.0497\n",
      "Epoch : [9/100], Loss : 0.9638\n",
      "Epoch : [10/100], Loss : 0.8686\n",
      "Epoch : [11/100], Loss : 0.7796\n",
      "Epoch : [12/100], Loss : 0.7068\n",
      "Epoch : [13/100], Loss : 0.6545\n",
      "Epoch : [14/100], Loss : 0.6201\n",
      "Epoch : [15/100], Loss : 0.5974\n",
      "Epoch : [16/100], Loss : 0.5792\n",
      "Epoch : [17/100], Loss : 0.5604\n",
      "Epoch : [18/100], Loss : 0.5390\n",
      "Epoch : [19/100], Loss : 0.5157\n",
      "Epoch : [20/100], Loss : 0.4930\n",
      "Epoch : [21/100], Loss : 0.4726\n",
      "Epoch : [22/100], Loss : 0.4556\n",
      "Epoch : [23/100], Loss : 0.4406\n",
      "Epoch : [24/100], Loss : 0.4264\n",
      "Epoch : [25/100], Loss : 0.4127\n",
      "Epoch : [26/100], Loss : 0.3998\n",
      "Epoch : [27/100], Loss : 0.3889\n",
      "Epoch : [28/100], Loss : 0.3800\n",
      "Epoch : [29/100], Loss : 0.3713\n",
      "Epoch : [30/100], Loss : 0.3614\n",
      "Epoch : [31/100], Loss : 0.3501\n",
      "Epoch : [32/100], Loss : 0.3388\n",
      "Epoch : [33/100], Loss : 0.3290\n",
      "Epoch : [34/100], Loss : 0.3212\n",
      "Epoch : [35/100], Loss : 0.3140\n",
      "Epoch : [36/100], Loss : 0.3062\n",
      "Epoch : [37/100], Loss : 0.2973\n",
      "Epoch : [38/100], Loss : 0.2888\n",
      "Epoch : [39/100], Loss : 0.2813\n",
      "Epoch : [40/100], Loss : 0.2746\n",
      "Epoch : [41/100], Loss : 0.2672\n",
      "Epoch : [42/100], Loss : 0.2591\n",
      "Epoch : [43/100], Loss : 0.2512\n",
      "Epoch : [44/100], Loss : 0.2444\n",
      "Epoch : [45/100], Loss : 0.2379\n",
      "Epoch : [46/100], Loss : 0.2310\n",
      "Epoch : [47/100], Loss : 0.2238\n",
      "Epoch : [48/100], Loss : 0.2173\n",
      "Epoch : [49/100], Loss : 0.2114\n",
      "Epoch : [50/100], Loss : 0.2053\n",
      "Epoch : [51/100], Loss : 0.1988\n",
      "Epoch : [52/100], Loss : 0.1926\n",
      "Epoch : [53/100], Loss : 0.1871\n",
      "Epoch : [54/100], Loss : 0.1817\n",
      "Epoch : [55/100], Loss : 0.1763\n",
      "Epoch : [56/100], Loss : 0.1713\n",
      "Epoch : [57/100], Loss : 0.1669\n",
      "Epoch : [58/100], Loss : 0.1627\n",
      "Epoch : [59/100], Loss : 0.1583\n",
      "Epoch : [60/100], Loss : 0.1544\n",
      "Epoch : [61/100], Loss : 0.1507\n",
      "Epoch : [62/100], Loss : 0.1472\n",
      "Epoch : [63/100], Loss : 0.1437\n",
      "Epoch : [64/100], Loss : 0.1404\n",
      "Epoch : [65/100], Loss : 0.1375\n",
      "Epoch : [66/100], Loss : 0.1346\n",
      "Epoch : [67/100], Loss : 0.1317\n",
      "Epoch : [68/100], Loss : 0.1291\n",
      "Epoch : [69/100], Loss : 0.1267\n",
      "Epoch : [70/100], Loss : 0.1243\n",
      "Epoch : [71/100], Loss : 0.1220\n",
      "Epoch : [72/100], Loss : 0.1199\n",
      "Epoch : [73/100], Loss : 0.1179\n",
      "Epoch : [74/100], Loss : 0.1159\n",
      "Epoch : [75/100], Loss : 0.1140\n",
      "Epoch : [76/100], Loss : 0.1123\n",
      "Epoch : [77/100], Loss : 0.1106\n",
      "Epoch : [78/100], Loss : 0.1089\n",
      "Epoch : [79/100], Loss : 0.1073\n",
      "Epoch : [80/100], Loss : 0.1059\n",
      "Epoch : [81/100], Loss : 0.1044\n",
      "Epoch : [82/100], Loss : 0.1030\n",
      "Epoch : [83/100], Loss : 0.1017\n",
      "Epoch : [84/100], Loss : 0.1005\n",
      "Epoch : [85/100], Loss : 0.0993\n",
      "Epoch : [86/100], Loss : 0.0981\n",
      "Epoch : [87/100], Loss : 0.0971\n",
      "Epoch : [88/100], Loss : 0.0960\n",
      "Epoch : [89/100], Loss : 0.0950\n",
      "Epoch : [90/100], Loss : 0.0940\n",
      "Epoch : [91/100], Loss : 0.0931\n",
      "Epoch : [92/100], Loss : 0.0922\n",
      "Epoch : [93/100], Loss : 0.0913\n",
      "Epoch : [94/100], Loss : 0.0905\n",
      "Epoch : [95/100], Loss : 0.0897\n",
      "Epoch : [96/100], Loss : 0.0889\n",
      "Epoch : [97/100], Loss : 0.0881\n",
      "Epoch : [98/100], Loss : 0.0874\n",
      "Epoch : [99/100], Loss : 0.0867\n",
      "Epoch : [100/100], Loss : 0.0861\n"
     ]
    }
   ],
   "source": [
    "# 위에서 정의한 클래스를 인스턴스화 시킴\n",
    "# model = NeuralNet(input_size, hidden_size, num_classes).to(device) # to(device) : 이 모델을 gpu 서버에서 돌린다는 뜻\n",
    "model = NeuralNet(input_size, hidden_size, num_classes)\n",
    "# loss, optimizer를 선정의\n",
    "loss_function = nn.CrossEntropyLoss() # Loss 기능 안에 Softmax 함수 기능 포함되어져 있다.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(num_epochs): # 5번\n",
    "    # Forward Pass\n",
    "    pred = model(X_train)\n",
    "    loss = loss_function(pred, y_train)\n",
    "    \n",
    "    loss_list.append(loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch : [{epoch + 1}/{num_epochs}], Loss : {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  학습을 마친 최종적인 모델의 값을 저장. model.ckpt 파일로 저장합니다.\n",
    "# torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch(학습)에 따른 Loss감소를 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epoch')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(num_epochs), loss_list)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습을 통해서 Loss를 감소시켰다면 이제는Test를 해봅니다.\n",
    "    테스트 할때는 학습의 의미가 없기때문에 Gradient Descent를 사용하지 않도록 합니다.\n",
    "    그 결과로 컴퓨터 Performance를 높이는 결과를 가져옵니다.\n",
    "    이때 우리가 테스트하는 데이타는 이미지가 아니고 단순 숫자 값으로 입력된다는 점을 잘 고려해야합니다.\n",
    "    출력된 값 중에서 가장 높은 값의 인덱스가 바로 target의 라벨이 됩니다.\n",
    "    \n",
    "    예측한 값과 정답을 일일이 비교해서 출력하고\n",
    "    총 30개의 Test 데이타 중에서 정확하게 맞춘 갯수를 최종적으로 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) 0 , 0\n",
      "2) 2 , 2\n",
      "3) 1 , 1\n",
      "4) 1 , 1\n",
      "5) 0 , 0\n",
      "6) 1 , 1\n",
      "7) 0 , 0\n",
      "8) 0 , 0\n",
      "9) 2 , 2\n",
      "10) 1 , 1\n",
      "11) 2 , 2\n",
      "12) 2 , 2\n",
      "13) 2 , 2\n",
      "14) 1 , 1\n",
      "15) 0 , 0\n",
      "16) 0 , 0\n",
      "17) 0 , 0\n",
      "18) 1 , 1\n",
      "19) 1 , 1\n",
      "20) 2 , 2\n",
      "21) 0 , 0\n",
      "22) 2 , 2\n",
      "23) 1 , 1\n",
      "24) 2 , 2\n",
      "25) 2 , 2\n",
      "26) 1 , 1\n",
      "27) 1 , 1\n",
      "28) 0 , 0\n",
      "29) 2 , 2\n",
      "30) 0 , 0\n",
      "30개의 Test 데이터 중에서 정답을 맞춘 개수는 30개 입니다!!\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): # 미분 안하겠다...실제로 학습할 필요가 없을 때 이 구문을 반드시 작성\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, data in enumerate(X_test):\n",
    "        y_val = model.forward(data)\n",
    "        print(f\"{i+1}) {str(y_val.argmax().item())} , {y_test[i]}\")\n",
    "\n",
    "        if y_val.argmax().item() == y_test[i]:\n",
    "            correct += 1\n",
    "    print(f\"30개의 Test 데이터 중에서 정답을 맞춘 개수는 {correct}개 입니다!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
